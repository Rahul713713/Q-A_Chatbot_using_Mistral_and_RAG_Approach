{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c9c7cfff450473c8c915fc30784d491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_756634bbdce04a898cd8195c9869950c",
              "IPY_MODEL_2b1db2f27c8f4375b21ca7a1ed6f66bb",
              "IPY_MODEL_7f7045b3b7a342e0a0e0396d7f320d51"
            ],
            "layout": "IPY_MODEL_37e32ab31ba74906aeff721c1552c8d4"
          }
        },
        "756634bbdce04a898cd8195c9869950c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58c73c1f1e647398c37028b13f379cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e8e25f2ae964491b9ae0a31ec73b74ca",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2b1db2f27c8f4375b21ca7a1ed6f66bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec06621332694b7f88c81de4b26d35ec",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f974bb7237ce4954b7244c462ebecbfa",
            "value": 8
          }
        },
        "7f7045b3b7a342e0a0e0396d7f320d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5038d37e2f6c4541bbe5b16005da2470",
            "placeholder": "​",
            "style": "IPY_MODEL_1d3ef918b54943f7807695f285c5fb9f",
            "value": " 8/8 [01:06&lt;00:00,  7.19s/it]"
          }
        },
        "37e32ab31ba74906aeff721c1552c8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d58c73c1f1e647398c37028b13f379cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e25f2ae964491b9ae0a31ec73b74ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec06621332694b7f88c81de4b26d35ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f974bb7237ce4954b7244c462ebecbfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5038d37e2f6c4541bbe5b16005da2470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d3ef918b54943f7807695f285c5fb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Connect the Colab instance to google drive**"
      ],
      "metadata": {
        "id": "a43LLMOKI4bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uka1k-jYIynL",
        "outputId": "8bdf5671-0584-4b10-975e-76f5aa0970e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround to avoid following error at notebook\n",
        "# NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "ToP6D-GaBu1R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install all required libraries**\n",
        "\n",
        "1. Huggingface libraries to use Mistral 7B LLM (Open Source LLM)\n",
        "\n",
        "2. LangChain library to call LLM to generate reponse based on the prompt"
      ],
      "metadata": {
        "id": "1aTJ_6UldPjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EHQaMxXHJlBD"
      },
      "outputs": [],
      "source": [
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "# LangChain related libraries\n",
        "!pip install -q -U langchain\n",
        "\n",
        "# Open-source pure-python PDF library capable of splitting, merging, cropping,\n",
        "# and transforming the pages of PDF files\n",
        "!pip install -q -U pypdf\n",
        "\n",
        "# Python framework for state-of-the-art sentence, text and image embeddings.\n",
        "!pip install -q -U sentence-transformers\n",
        "\n",
        "# FAISS Vector Databses specific Libraries\n",
        "!pip install -q -U faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing all the required libraries and checking whether we have access to GPU or not. Here, we are using the T4 GPU provided by colab.\n",
        "\n",
        "We must be getting following output after running the cell\n",
        "\n",
        "Device: cuda\n",
        "\n",
        "Tesla T4"
      ],
      "metadata": {
        "id": "Gt43HfdAdvjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the libraries\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "\n",
        "import torch\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "tYB9unhcKZnp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPlhKwW2LJOX",
        "outputId": "cb8811fd-c1d5-41ca-96c0-faf0d7af6e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **We are using Mistral 7 Billion parameter LLM (Open source from HuggingFace)**"
      ],
      "metadata": {
        "id": "NR-fee8reJXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Mitsral 7B model and create an instance of the model and tokenier from Huggingface\n",
        "\n",
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained (model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ],
      "metadata": {
        "id": "pB86JDv3JOU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6c9c7cfff450473c8c915fc30784d491",
            "756634bbdce04a898cd8195c9869950c",
            "2b1db2f27c8f4375b21ca7a1ed6f66bb",
            "7f7045b3b7a342e0a0e0396d7f320d51",
            "37e32ab31ba74906aeff721c1552c8d4",
            "d58c73c1f1e647398c37028b13f379cf",
            "e8e25f2ae964491b9ae0a31ec73b74ca",
            "ec06621332694b7f88c81de4b26d35ec",
            "f974bb7237ce4954b7244c462ebecbfa",
            "5038d37e2f6c4541bbe5b16005da2470",
            "1d3ef918b54943f7807695f285c5fb9f"
          ]
        },
        "outputId": "eabc853d-3e29-484b-bd10-e34384187880"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9c7cfff450473c8c915fc30784d491"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating pipelines to run LLM at Colab notebook**\n"
      ],
      "metadata": {
        "id": "jJkALiFLiLxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a pipeline object for the model\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,\n",
        "    temperature = 0.3,\n",
        "    do_sample=True,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "MpPNufTqJ48O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try running the Mistral 7B model once\n",
        "text = \"What is the future of AI?\"\n",
        "response = mistral_llm.invoke(text)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLKNAZSdLz6Y",
        "outputId": "73f9bae5-9516-41f9-8503-cd83f5154235"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A: The future of AI is difficult to predict, but it is likely that AI will continue to play an increasingly important role in our lives. Some possible developments include more advanced machine learning algorithms, greater integration of AI into everyday devices and systems, and the creation of truly autonomous machines capable of making decisions on their own. However, there are also concerns about the potential risks and ethical implications of these developments, and it remains to be seen how society will respond to them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the PDF and create chunk of texts**\n"
      ],
      "metadata": {
        "id": "PGffkOIljoRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pdf file\n",
        "loader = PyPDFLoader('/content/drive/MyDrive/Colab Notebooks/Attention-is-all-you-need-Paper.pdf')\n",
        "documents = loader.load()\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "chunked_docs  = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Con-VMP5h0N1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace embeddings to create embedding of chunked docs to store at\n",
        "# Vector Database\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "PXHeKBbVIT6m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store chunk of pdf files at FAISS vector database by using HuggingFace Embedding model\n",
        "\n",
        "faiss_db = FAISS.from_documents(chunked_docs,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))"
      ],
      "metadata": {
        "id": "Tqklqp4Lh4H9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect query to FAISS index using a retriever\n",
        "retriever = faiss_db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ],
      "metadata": {
        "id": "dKA71GbSIz5H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Conversational Retrieval Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(mistral_llm, retriever,return_source_documents=True)"
      ],
      "metadata": {
        "id": "N-x13o1VmOyO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the answer of question from Vector Database\n",
        "import sys\n",
        "\n",
        "def get_user_input():\n",
        "    return input('Prompt: ').lower()\n",
        "\n",
        "def main():\n",
        "    chat_history = []\n",
        "\n",
        "    query = get_user_input()\n",
        "\n",
        "    result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n",
        "    print(f'Answer: {result[\"answer\"]}\\n')\n",
        "\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw45rBBGmQ_w",
        "outputId": "33224d97-81ae-438f-8650-66696fc557be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What do you mean by transformers and the attention mechanism?\n",
            "Answer:  Transformers are a type of deep learning architecture that are based on attention mechanisms. Attention mechanisms are a way of computing a weighted combination of inputs, where the weights are determined by a compatibility function. In the context of transformers, attention is used to compute a weighted combination of inputs in order to extract relevant information from the input sequence. The attention mechanism is a fundamental component of transformers and is what sets them apart from other deep learning architectures.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XagRPidIR225"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}